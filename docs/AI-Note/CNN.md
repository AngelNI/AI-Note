# CNN

## 1  卷积神经网络

卷积神经网络是一种前馈型神经网络, 受生物自然视觉认知机制启发而来的. 现在, CNN 已经成为众多科学领域的研究热点之一, 特别是在模式分类领域, 由于该网络避免了对图像的复杂前期预处理, 可以直接输入原始图像, 因而得到了更为广泛的应用. 可应用于图像分类, 目标识别, 目标检测, 语义分割等等.

<img src="https://s2.ax1x.com/2019/08/15/mVa7ss.png" alt="" border="0" />

图中是一个图形识别的CNN模型。可以看出最左边的船的图像就是我们的输入层，计算机理解为输入若干个矩阵

接着是卷积层（Convolution Layer）,这个是CNN特有的，我们后面专门来讲。卷积层的激活函数使用的是ReLU。我们在DNN中介绍过ReLU的激活函数，它其实很简单，就是ReLU(x)=max(0,x)。在卷积层后面是池化层(Pooling layer)，这个也是CNN特有的，需要注意的是，池化层没有激活函数。

卷积层+池化层的组合可以在隐藏层出现很多次，上图中出现两次。而实际上这个次数是根据模型的需要而来的。当然我们也可以灵活使用使用卷积层+卷积层，或者卷积层+卷积层+池化层的组合，这些在构建模型的时候没有限制。但是最常见的CNN都是若干卷积层+池化层的组合，如上图中的CNN结构。

在若干卷积层+池化层后面是全连接层（Fully Connected Layer, 简称FC），全连接层类似DNN结构，只是输出层使用了Softmax激活函数来做图像识别的分类，

从上面CNN的模型描述可以看出，比较特殊的是卷积层和池化层，只要把卷积层和池化层的原理搞清楚了，那么搞清楚CNN就容易很多了。

**CNN基本组成结构**

<img src="https://s2.ax1x.com/2019/08/15/mVavJU.png" alt="" border="0">

1.输入层：数据的输入，但不是一维的向量，而是保留图片结构

2.卷积层：使用固定的尺寸和权值的卷积核（也称滤波器），以一定的步长移动，对输入的矩阵进行滤波，加偏执，最后激活，最终组成一个特征图

3.池化层：类比卷积核，以固定尺寸，一定步长移动，用最大池化或者平均池化对卷积层得到的特征进行差采样，加偏置，激活，组成一个更小的特征图

4.全链接层：在CNN尾部进行拟合，减少特征值丢失

5.输出层：输出结果

### 1.1.  卷积层 Convolution Layter

以一张图片为例，以像素值为元素，其中每个像素点都是0-255的数值，根据三原色RGB三通道，将一张图片分为 m * n * k的三维矩阵，其中m 是高度，n 是宽度，k 是深度，这样图片就可以被计算机识别了。如下图。

<img src="https://s2.ax1x.com/2019/08/15/mVdZWD.jpg" alt="" border="0" />

拿其中一维为例，比如有一个5 * 5的矩阵，使用一个3 * 3的卷积核进行卷积，最后会得到一个3 * 3的特征图，卷积核也叫滤波器

<img src="https://s2.ax1x.com/2019/08/15/mVdnQH.png" alt="" border="0" />

**特征图大小计算**

设输入的矩阵大小为w * w，卷积核大小为k * k，步幅为s，补零层数为p，则特征图大小为

 特征图大小 =（w + 2 * p - k）/ s+1
 

**具体操作如图所示**

<img src="https://s2.ax1x.com/2019/08/15/mVdJfS.gif" alt="" border="0" />

<img src="https://s2.ax1x.com/2019/08/15/mVdUyj.gif" alt="" border="0" />

举个例子如下，图中的输入是一个二维的3x4的矩阵，而卷积核是一个2x2的矩阵。这里我们假设卷积是一次移动一个像素来卷积的，那么首先我们对输入的左上角2x2局部和卷积核卷积，即各个位置的元素相乘再相加，得到的输出矩阵S的S00的元素，值为aw+bx+ey+fz。接着我们将输入的局部向右平移一个像素，现在是(b,c,f,g)四个元素构成的矩阵和卷积核来卷积，这样我们得到了输出矩阵S的S01的元素，同样的方法，我们可以得到输出矩阵S的S02，S10，S11，S12的元素。

<img src="https://s2.ax1x.com/2019/08/15/mVdgl4.png" alt="" border="0" />

### 1.2  步长Stride

从具体操作中，我们可以发现卷积核每次移动的距离是不同的，这里把卷积核移动的距离称之为步长。

我们不难发现，不同的步长卷积的效率不同，这样我们可以，我们可以通过对步长的控制，来提高对大的图片卷积的效率。

<img src="https://s2.ax1x.com/2019/08/15/mVdHpD.gif" alt="" border="0">

<img src="https://s2.ax1x.com/2019/08/15/mVdLXd.gif" alt="" border="0">

### 1.3  补零（边缘扩展） Padding

作用：

  1.  为了不丢弃原图信息
  2. 为了保持特征图大小与原图一样
  3. 为了更深层的layer的input拥有足够大的信息量
  4. 补零层的值都是0，没有噪声。

方便从初始位置以步长为单位移动到边缘位置

两种方式：

    1. SAME:补上
    2. VALID:丢弃

补零层是一个超参数，补零层的设置要根据卷积核的大小、步幅，输入矩阵的大小进行调整，以使卷积层滑到边缘

注：  超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。

**高维卷积**

通常把一张图片分解为根据r g b分为三维的矩阵，此时输入的矩阵是3维，卷积层也相应的变为3层

如图

<img src="https://s2.ax1x.com/2019/08/15/mV0UZn.gif" alt="" border="0" />

最左边的是输入的特征图矩阵，深度为3，补零层数为1，移动步幅为2

在这里，每次滑动会得到三个数，这三个数之和作为卷积层的输出，等到最右边两个特征图

### 1.4  池化层

卷层得到的特征图需要通过池化降低数据数量

池化方法一般为 最大值池化和平局值池化

<img src="https://s2.ax1x.com/2019/08/15/mV0rzF.png" alt="" border="0" />

<img src="https://s2.ax1x.com/2019/08/15/mVBies.png" alt="" border="0" />

和卷积一样, 池化也有一个滑动的核, 可以称之为滑动窗口, 上图中滑动窗口的大小为 2×2, 步幅为 2, 每滑动到一个区域, 则取最大值作为输出, 这样的操作称为 Max Pooling. 还可以采用输出均值的方式, 称为 Mean Pooling.

## 2. 卷积神经网络前向传导

### 2.1隐藏层-->卷积层

　	我们这里还是以图像识别为例。

　　先考虑最简单的，样本都是二维的黑白图片。这样输入层X就是一个矩阵，矩阵的值等于图片的各个像素位置的值。这时和卷积层相连的卷积核W就也是矩阵。

　　如果样本都是有RGB的彩色图片，这样输入X就是3个矩阵，即分别对应R，G和B的矩阵，或者说是一个张量。这时和卷积层相连的卷积核W就也是张量，对应的最后一维的维度为3.即每个卷积核都是3个子矩阵组成。

　　同样的方法，对于3D的彩色图片之类的样本，我们的输入X可以是4维，5维的张量，那么对应的卷积核W也是个高维的张量。

　　不管维度多高，对于我们的输入，前向传播的过程可以表示为：

<img src="https://s2.ax1x.com/2019/08/15/mVs00J.png" alt="" border="0" />

其中，l=1,2,...,L−1l=1,2,...,L−1，LL为神经网络的层数

其中，上标代表层数，而b代表我们的偏置项,w卷积核权重， f  为激活函数，这里一般都是ReLU。

假设有如下矩阵：


$$
\begin{equation*}
A=\left[\begin{array}{ccc}
1 & 2 & 3\\
4 & 5 & 6\\
7 & 8 & 9
\end{array}\right]\quad
B=\begin{bmatrix}
1 & 2\\
3 & 4
\end{bmatrix}
\end{equation*}
$$


那么B卷积A的结果就是让 B 在矩阵 A 上滑动，换言之，就是 B 与 A 的所有2 × 2连续子矩阵做“对应元素积之和”运算，所以，此时的结果 C 应该为：


$$
\begin{equation*}
C=\begin{bmatrix}
37 & 47\\
67 & 77
\end{bmatrix}
\end{equation*}
$$


在这里，卷积层我们需要定义卷积核的个数K，卷积核子矩阵的维度F，填充大小P以及步幅S。

### 2.2  卷积层-->池化层

假设矩阵C为6×46×4的矩阵，池化窗口为2×22×2，则按照池化窗口大小将矩阵C分割成6块不相交的2×22×2小矩阵，对对每个块中的所有元素做求和平均操作，称为平均池化，取最大 值则称为最大池化。得到的矩阵S称为pool map。如：


$$
\begin{equation*}
C=\begin{bmatrix}
1 & 2 & 3 & 4\\
1 & 2 & 3 & 4\\
5 & 6 & 7 & 8\\
5 & 6 & 7 & 8\\
9 & 0 & 1 & 2\\
9 & 0 & 1 & 2
\end{bmatrix}
\end{equation*}
$$


若平均池化，则：


$$
\begin{equation*}
S=\begin{bmatrix}
1.5 & 3.5\\
5.5 & 7.5\\
4.5 & 1.5
\end{bmatrix}
\end{equation*}
$$


若最大池化，则：


$$
\begin{equation*} S=\begin{bmatrix} 2 & 4\\ 6 & 8\\ 9 & 2 \end{bmatrix} \end{equation*}
$$


由于池化也称为下采样，用S=down(C)S=down(C)表示，为了使得池化层具有可学习性，一般令：


$$
S = \beta down(C) + b
$$


这里需要需要我们定义的CNN模型参数是：

　　　　（1）池化区域的大小k

　　　　（2）池化的标准，一般是MAX或者Average。

### 2.3  全链接层

经过了若干全连接层之后，最后的一层为Softmax输出层。此时输出层和普通的全连接层唯一的区别是，激活函数是softmax函数。

这里需要需要我们定义的CNN模型参数是：

　　　　（1）全连接层的激活函数

　　　　（2）全连接层各层神经元的个数

**总结**

<img src="https://s2.ax1x.com/2019/08/15/mV67y8.png" alt="" border="0" />

## 3.反向传导

卷积神经网络的反向传播本质上是和BP神经网络是一致的，区别在于全连接和非全连接：在反向求导时，卷积神经网络要明确参数连接了哪些神经元；而全连接的普通神经网络中的相邻两层的神经元都是与另一层的所有神经元相连的，因此反向求导时非常简单。

**全连接层** 全连接层的反向求导是与普通神经网络的反向求导是一致的：


$$
\frac{\partial J}{\partial w^{(l)}} = \delta^{(l)}(a^{(l-1)})^T \\
\frac{\partial J}{\partial b^{(l)}} =  \delta^{(l)}
$$


**卷积层** 假设当前卷积层为ll，下一层为池化层l+1l+1，上一层也为池化层l−1l−1。那么从l−1l−1层到ll层有：


$$
a_i^{(l)} = f(u_i^{(l)}) = f(\sum_{j=1}^{N_{l-1}}conv2(a_j^{(l-1)}, K_{ij}^{(l)})+b_{ij}^{(l)})
$$


其中，Nl−1为l−1层pool maps的个数。如，当l=1时，Nl−1=1；当l=3时，Nl−1=F1。




为了求得卷积层ll的各个神经元的δδ，关键是要必须弄清楚该神经元与l+1l+1层中的哪些神经元连接，因为求该神经元的δδ时，只与这些神经元相关。递推的方式与全连接的神经网络的不同之处在于：

1. 卷积层 l 的各个神经元的 δ 只和 l+1 层的相关神经元有关
2. 卷积层 l 到池化层 l+1 做了下采样运算，使得矩阵维度减小，因此，δ(l+1) 需要上采样up成卷积层的矩阵维度。定义 up 运算为（若上采样窗口为2×2）：


$$
up(\begin{bmatrix}1&2\\3&4\end{bmatrix}) = \begin{bmatrix}1&1&2&2\\1&1&2&2\\3&3&4&4\\3&3&4&4\end{bmatrix}
$$


因此，有：


$$
\delta_i^{(l)} = \beta_i^{(l+1)} (a(u_i^{(l)}) \circ up(\delta_i^{l+1}))  \\
\frac{\partial J}{\partial b_i^{(l)}} = \sum_{s,t}(\delta_i)_{st}  \\
\frac{\partial J}{\partial K_{ij}^{(l)}} = \sum_{st}(\delta_i^{(l)})_{st}(P_j^{(l-1)})_{st}
$$

$$
其中，(*)_{st}遍历∗的所有元素，(P_j^{(l-1)})_{st}是(\delta_i^{(l)})所连接的 l−1 层中a_j^{l-1}中相关的元素构成的矩阵。
$$


**池化层** 假设当前池化层为 ll，下一层为全连接层，那么当前池化层就是全连接层的输入，可以根据全连接层的 BP 求导公式递推算出。因此只需讨论下一层 l+1l+1 为卷积层的情形，上一层 l−1l−1也为卷积层，该情形下有：


$$
a_i^{(l)} = f( \beta_i^{(l)} down(a_i^{(l-1)}) + b_i^{(l)} )
$$


同样地，为了求得池化层 l 的各个神经元的 δ，关键是要必须弄清楚该神经元与 l+1层中的哪些神经元连接，因为求该神经元的δ时，只与这些神经元相关。递推的方式与全 连接的神经网络的不同之处在于：

1. 池化层 l 的各个神经元的δ只和 l+1 层的相关神经元有关
2. 池化层 l 到卷积层 l+1 做了窄卷积运算，使得矩阵维度减小，因此，δil+1 需要与相应的卷积核做宽卷积运算使得矩阵维度扩展回去。 因此，有：


$$
\delta_i^{(l)} = \sum_{j=1}^{N_l}a_i^{(l)} \circ conv2(\delta_{j}^{(l+1)}, K_{ji}^{(l+1)}, 'full')  \\
\frac{\partial J}{\partial b_i^{(l)}} = \sum_{s,t}(\delta_i^{(l)})_{st}   \\
\frac{\partial J}{\partial \beta_i^{(l)}} = \sum_{s,t}( \delta_i^{(l)} \circ d_i^{(l-1)} )_{st}
$$



$$
其中，(*)_{st}遍历∗的所有元素，d_i^{(l-1)} = down(a_i^{(l-1)})。
$$


[参考资料](http://tech.youmi.net/2016/07/163347168.html)

## 4. 经典的神经网络



### 4.1  LeNet

LeNet诞生于1994年，由深度学习三巨头之一的Yan LeCun提出，他也被称为卷积神经网络之父。LeNet主要用来进行手写字符的识别与分类，准确率达到了98%，并在美国的银行中投入了使用，被用于读取北美约10%的支票。LeNet奠定了现代卷积神经网络的基础。

**网络结构**

<img src="https://s2.ax1x.com/2019/08/15/mVRb3n.png" alt="" border="0" />

上图为LeNet结构图，是一个6层网络结构：三个卷积层，两个下采样层和一个全连接层（图中C代表卷积层，S代表下采样层，F代表全连接层）。其中，C5层也可以看成是一个全连接层，因为C5层的卷积核大小和输入图像的大小一致，都是5*5

**网络特点**

1. 每个卷积层包括三部分：卷积、池化和非线性激活函数（sigmoid激活函数）
2. 使用卷积提取空间特征
3. 降采样层采用平均池化

### 4.2  AlexNet

**网络背景**

AlexNet由Hinton的学生Alex Krizhevsky于2012年提出，并在当年取得了Imagenet比赛冠军。AlexNet可以算是LeNet的一种更深更宽的版本，证明了卷积神经网络在复杂模型下的有效性，算是神经网络在低谷期的第一次发声，确立了深度学习，或者说卷积神经网络在计算机视觉中的统治地位。

**网络结构**

<img src="https://s2.ax1x.com/2019/08/15/mVWp4J.png" alt="" border="0" />

原作者在训练 AlexNet 时，一个 GPU 负责图中顶层部分，一个 GPU 运行图中底层部分，GPU 之间仅在某些层互相通信。5 个卷积层，5 个池化层，3 个全连接层，大约 5000 万个可调参数组成了这个经典的卷积神经网络。最后的全连接层输出到 1000 维的 softmax 层，产生一个覆盖 1000 类标记的分布。最终完成了对 ImageNet 的分类任务。

**网络特点**

１．　使用两块GPU并行加速训练，大大降低了训练时间

２．　成功使用ReLu作为激活函数，解决了网络较深时的梯度弥散问题

３．　使用数据增强、dropout和LRN层来防止网络过拟合，增强模型的泛化能力

### 4.3  VGGNet

**网络背景**

VGGNet是牛津大学计算机视觉组和Google DeepMind公司一起研发的深度卷积神经网络，并取得了2014年Imagenet比赛定位项目第一名和分类项目第二名。该网络主要是泛化性能很好，容易迁移到其他的图像识别项目上，可以下载VGGNet训练好的参数进行很好的初始化权重操作，很多卷积神经网络都是以该网络为基础，比如FCN，UNet，SegNet等。vgg版本很多，常用的是VGG16，VGG19网络。

**网络结构**

<img src="https://s2.ax1x.com/2019/08/15/mVWmUe.png" alt="" border="0" />

上图为VGG16的网络结构，共16层（不包括池化和softmax层），所有的卷积核都使用3*3的大小，池化都使用大小为2*2，步长为2的最大池化，卷积层深度依次为64 -> 128 -> 256 -> 512 ->512。

**网络特点**

网络结构和AlexNet有点儿像，不同的地方在于：

主要的区别，一个字：深，两个字：更深。把网络层数加到了16-19层（不包括池化和softmax层），而AlexNet是8层结构。
将卷积层提升到卷积块的概念。卷积块有2~3个卷积层构成，使网络有更大感受野的同时能降低网络参数，同时多次使用ReLu激活函数有更多的线性变换，学习能力更强
在训练时和预测时使用Multi-Scale做数据增强。训练时将同一张图片缩放到不同的尺寸，在随机剪裁到224*224的大小，能够增加数据量。预测时将同一张图片缩放到不同尺寸做预测，最后取平均值。

### 1.7.4  ResNet



**网络背景**

ResNet（残差神经网络）由微软研究院的何凯明等4名华人于2015年提出，成功训练了152层超级深的卷积神经网络，效果非常突出，而且容易结合到其他网络结构中。在五个主要任务轨迹中都获得了第一名的成绩：

1. ImageNet分类任务：错误率3.57%
2. ImageNet检测任务：超过第二名16%
3. ImageNet定位任务：超过第二名27%
4. COCO检测任务：超过第二名11%
5. COCO分割任务：超过第二名12%

**网络结构**

<img src="https://s2.ax1x.com/2019/08/15/mVWaCj.png" alt="" border="0" />

**网络特点**

使得训练超级深的神经网络成为可能，避免了不断加深神经网络，准确率达到饱和的现象（后来将层数增加到1000层）
输入可以直接连接到输出，使得整个网络只需要学习残差，简化学习目标和难度。
ResNet是一个推广性非常好的网络结构，容易和其他网络结合

### 4.5  历史发展

<img src="https://s2.ax1x.com/2019/08/15/mVW0vq.png" alt="" border="0" />