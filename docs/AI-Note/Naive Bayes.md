# Naive Bayes

## 1.  摘要

贝叶斯分类器是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类器。

朴素贝叶斯分类器是贝叶斯分类器中最简单，也是最常见的一种分类方法。并且，朴素贝叶斯算法仍然是流行的十大挖掘算法之一，该算法是有监督的学习算法，解决的是分类问题。

**优点**在于简单易懂、学习效率高、分类过程中时空开销小、在某些领域的分类问题中能够与决策树、神经网络相媲美。

**缺点**由于该算法以自变量之间的独立（条件特征独立）性和连续变量的正态性假设为前提，就会导致算法精度在某种程度上受影响。

## 2.应用

实时预测：毫无疑问，朴素贝叶斯很快。

多类预测：这个算法以多类别预测功能闻名，因此可以用来预测多类目标变量的概率。

文本分类/垃圾邮件过滤/情感分析：

1. 相比较其他算法，朴素贝叶斯的应用主要集中在文本分类（变量类型多，且更独立），具有较高的成功率。因此被广泛应用于垃圾邮件过滤（识别垃圾邮件）和情感分析（在社交媒体平台分辨积极情绪和消极情绪的用户）。

推荐系统：朴素贝叶斯分类器和协同过滤结合使用可以过滤出用户想看到的和不想看到的东西。

## 3  分类基础

### 3.1  先验概率

是指根据以往经验和分析得到的概率。

举个例子：如果我们对西瓜的色泽、根蒂和纹理等特征一无所知，按照常理来说，西瓜是好瓜的概率是60%。那么这个概率P（好瓜）就被称为先验概率。

### 3.2  后验概率

事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小。

举个例子：假如我们了解到判断西瓜是否好瓜的一个指标是纹理。一般来说，纹理清晰的西瓜是好瓜的概率大一些，大概是75%。如果把纹理清晰当作一种结果，然后去推测好瓜的概率，那么这个概率P（好瓜|纹理清晰）就被称为后验概率。后验概率类似于条件概率。

### 3.3  朴素贝叶斯估计

贝叶斯估计（Bayesian estimation）是利用贝叶斯定理结合新的证据及以前的先验概率，来得到新的概率。它提供了一种计算假设概率的方法，基于假设的先验概率、给定假设下观察到不同数据的概率以及观察到的数据本身。

<img src="https://s2.ax1x.com/2019/08/15/mElZVJ.png" alt="" border="0" />

### 3.4  似然估计

在统计学中，似然函数（likelihood function，通常简写为likelihood，似然）是一个非常重要的内容，在非正式场合似然和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。

概率是在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性

似然，是在确定的结果下去推测产生这个结果的可能环境（参数），运用出现的结果来判断这个事情本身的性质（参数），也就是似然。

结果和参数相互对应的时候，似然和概率在数值上是相等的，



**最大似然函数**

最大似然估计，英文为Maximum Likelihood Estimation，简写为MLE，也叫极大似然估计，是用来估计概率模型参数的一种方法。最大似然估计的思想是使得观测数据（样本）发生概率最大的参数就是最好的参数。

对一个独立同分布的样本集来说，总体的似然就是每个样本似然的乘积。针对抛硬币的问题，似然函数可写作：![L(X;\theta)=\prod_{i=0}^nP(x_i|\theta)=\theta^6(1-\theta)^4](https://math.jianshu.com/math?formula=L(X%3B%5Ctheta)%3D%5Cprod_%7Bi%3D0%7D%5EnP(x_i%7C%5Ctheta)%3D%5Ctheta%5E6(1-%5Ctheta)%5E4)

根据最大似然估计，使![L(X;\theta)](https://math.jianshu.com/math?formula=L(X%3B%5Ctheta))取得最大值的![\theta](https://math.jianshu.com/math?formula=%5Ctheta)即为估计结果，令![L(X;\theta)\prime =0](https://math.jianshu.com/math?formula=L(X%3B%5Ctheta)%5Cprime%20%3D0)可得![\hat{\theta}=0.6](https://math.jianshu.com/math?formula=%5Chat%7B%5Ctheta%7D%3D0.6)。似然函数图如下：



![img](https:////upload-images.jianshu.io/upload_images/3232548-c785fe210be37ef5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/650/format/webp)

MLE

由于总体的似然就是每个样本似然的乘积，为了求解方便，我们通常会将似然函数转成对数似然函数，然后再求解。可以转成对数似然函数的主要原因是对数函数并不影响函数的凹凸性。因此上式可变为：

![lnL(X;\theta)=ln\prod_{i=0}^nP(x_i|\theta)=\sum_{i=0}^nln(P(x_i|\theta))=6ln(\theta)+4ln(1-\theta)](https://math.jianshu.com/math?formula=lnL(X%3B%5Ctheta)%3Dln%5Cprod_%7Bi%3D0%7D%5EnP(x_i%7C%5Ctheta)%3D%5Csum_%7Bi%3D0%7D%5Enln(P(x_i%7C%5Ctheta))%3D6ln(%5Ctheta)%2B4ln(1-%5Ctheta))

令      ![ln(L(X;\theta)\prime) =0](https://math.jianshu.com/math?formula=ln(L(X%3B%5Ctheta)%5Cprime)%20%3D0)

可得![\hat{\theta}=0.6](https://math.jianshu.com/math?formula=%5Chat%7B%5Ctheta%7D%3D0.6)。

## 4  贝叶斯定理

每次提到贝叶斯定理，我心中的崇敬之情都油然而生，倒不是因为这个定理多高深，而是因为它特别有用。这个定理解决了现实生活里经常遇到的问题：已知某条件概率，如何得到两个事件交换后的概率，也就是在已知P(A|B)的情况下如何求得P(B|A)。这里先解释什么是条件概率：

​       表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。其基本求解公式为：P(A|B)=P(AB)/P(B)

贝叶斯定理之所以有用，是因为我们在生活中经常遇到这种情况：我们可以很容易直接得出P(A|B)，P(B|A)则很难直接得出，但我们更关心P(B|A)，贝叶斯定理就为我们打通从P(A|B)获得P(B|A)的道路。

  直接给出贝叶斯定理：

<img src="https://s2.ax1x.com/2019/08/15/mElZVJ.png" alt="" border="0" />

其中,P(A|B)是指事件B发生的情况下事件A发生的概率(条件概率).在贝叶斯定理中,每个名词都有约定俗成的名称:

- P(A|B)是已知B发生后A的条件概率,也由于得知B的取值而被称作**A的后验概率**;
- P(A)是A的先验概率(或边缘概率).之所以称为"先验"是因为它不考虑任何B方面的因素;
- P(B|A)是已知A发生后B的条件概率,也由于得知A的取值而成称作**B的后验概率**;
- P(B)是B的先验概率(或边缘概率).

## 5.算法流程

<img src="https://s2.ax1x.com/2019/08/15/mE3pcV.png" alt="" border="0" />

## 6.  如何在Python中使用Naive Bayes构建基本模型？

同样，scikit learn（python库）将帮助在这里用Python构建Naive Bayes模型。在scikit学习库下有三种类型的Naive Bayes模型：

高斯： 它用于分类，它假设特征遵循正态分布。

多项式： 用于离散计数。例如，假设我们有文本分类问题。在这里我们可以考虑进一步的bernoulli试验，而不是“在文档中出现的单词”，我们“计算文档中出现单词的频率”，你可以把它想象为“观察到结果数x_i的次数”超过n次试验“。

伯努利： 如果你的特征向量是二元的（即零和1），二项式模型很有用。一个应用是具有“词袋”模型的文本分类，其中1和0分别是“文档中出现单词”和“文档中不出现单词”。

根据您的数据集，您可以选择上述任何一种模型。以下是高斯模型的示例。



## 7. 朴素贝叶斯的优点和缺点是什么？

**优点：**

1. 预测测试数据集类很容易，也很快。它在多类预测中也表现良好
2. 当独立性假设成立时，朴素贝叶斯分类器与其他模型（如逻辑回归）相比表现更好，您需要的训练数据更少。
3. 与数值变量相比，它在分类输入变量的情况下表现良好。对于数值变量，假设正态分布（钟形曲线，这是一个强有力的假设）。

**缺点：**

1. 如果分类变量具有在训练数据集中未观察到的类别（在测试数据集中），则模型将指定0（零）概率并且将不能进行预测。这通常被称为“零频率”。为了解决这个问题，我们可以使用平滑技术。最简单的平滑技术之一称为拉普拉斯估计。
2. 另一方面，朴素贝叶斯也被称为坏估计，因此来自predict_proba的概率输出不应过于严肃。
3. 朴素贝叶斯的另一个限制是独立预测因子的假设。在现实生活中，我们几乎不可能获得一组完全独立的预测变量。



## 8. 朴素贝叶斯算法的应用

1.实时预测： 朴素贝叶斯是一个渴望学习的分类器，它确实很快。因此，它可以用于实时预测。

2.多类预测： 该算法对于多类预测特征也是众所周知的。在这里，我们可以预测多类目标变量的概率。

3.文本分类/垃圾邮件过滤/情感分析： 主要用于文本分类的朴素贝叶斯分类器（由于更好地导致多类问题和独立性规则）与其他算法相比具有更高的成功率。

因此，它被广泛用于垃圾邮件过滤（识别垃圾邮件）和情感分析（在社交媒体分析中，以识别积极和消极的客户情绪）

4.推荐系统： 朴素贝叶斯分类器和协同过滤一起构建一个推荐系统，该系统使用机器学习和数据挖掘技术来过滤看不见的信息并预测用户是否想要给定的资源



## 9.  提高朴素贝叶斯模型力量的技巧

以下是提高Naive Bayes模型功效的一些技巧：

1. 如果连续特征没有正态分布，我们应该使用变换或不同方法将其转换为正态分布。

2. 如果测试数据集具有零频率问题，则应用平滑技术“拉普拉斯校正”来预测测试数据集的类别。

3. 删除相关要素，因为高度相关的要素在模型中被投票两次，并且可能导致过度膨胀的重要性。

4. 朴素贝叶斯分类器具有有限的参数调整选项，如alpha = 1用于平滑，fit_prior = [True | False]用于学习类先验概率和其他一些选项（在此处查看详细信息）。建议您专注于数据的预处理和功能选择。

5. 可能会考虑应用一些分类器组合技术，如 集成，装袋和助推，但这些方法无济于事。实际上，“集合，提升，装袋”无济于事，因为它们的目的是减少差异。朴素贝叶斯没有变化可以最小化。