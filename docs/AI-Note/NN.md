# NN

## 1.神经元

神经元是神经网络中最基本的结构，也可以说是神经网络的基本单元，它的设计灵感完全来源于生物学上神经元的信息传播机制。我们学过生物的同学都知道，神经元有两种状态：兴奋和抑制。一般情况下，大多数的神经元是处于抑制状态，但是一旦某个神经元收到刺激，导致它的电位超过一个阈值，那么这个神经元就会被激活，处于“兴奋”状态，进而向其他的神经元传播化学物质（其实就是信息）。

<img src="https://s2.ax1x.com/2019/08/15/mA2nsS.png" alt="" border="0">

## 2.M-P神经元模型

1943年，McCulloch和Pitts将上图的神经元结构用一种简单的模型进行了表示，构成了一种人工神经元模型，也就是我们现在经常用到的“M-P神经元模型”，如下图所示：

<img src="https://s2.ax1x.com/2019/08/15/mA2ULF.png" alt="" border="0" />

从上图M-P神经元模型可以看出，神经元的输出

<img src="https://s2.ax1x.com/2019/08/15/mA2Wee.png" alt="" border="0" />

其中θ为我们之前提到的神经元的激活阈值，函数f(⋅)也被称为是激活函数。如上图所示，函数f(⋅)可以用一个阶跃方程表示，大于阈值激活；否则则抑制。但是这样有点太粗暴，因为阶跃函数不光滑，不连续，不可导，因此我们更常用的方法是用sigmoid函数来表示函数函数f(⋅)。

**激活函数**

判定每个神经元的输出

通俗来说，激活函数一般是非线性函数，其作用是能够给神经网络加入一些非线性因素，使得神经网络可以更好地解决较为复杂的问题。

缺点：当输入非常大或者非常小的时候这些神经元的梯度接近于0的

## 3.  感知机和神经网络

感知机（perceptron）是由两层神经元组成的结构，输入层用于接受外界输入信号，输出层（也被称为是感知机的功能层）就是M-P神经元。下图表示了一个输入层具有三个神经元（分别表示为x0、x1、x2）的感知机结构：

<img src="https://s2.ax1x.com/2019/08/15/mARM6K.jpg" alt="" border="0" />

根据上图不难理解，感知机模型可以由如下公式表示：

y=f(wx+b)

## 4.激活函数

### 4.1  sigmoid

<img src="https://s2.ax1x.com/2019/08/15/mARTAJ.png" alt="" border="0" />

<img src="https://s2.ax1x.com/2019/08/15/mAWuNj.png" alt="mAWuNj.png" border="0" height=150 width = 200/>

Sigmoid 函数的取值范围在 (0,1) 之间，单调连续，求导容易，一般用于二分类神经网络的输出层。

缺点：

```
1.Sigmoid 函数饱和区范围广，容易造成梯度消失
2.参数矩阵 W 的每个元素都会朝着同一个方向变化，同为正或同为负。这对于神经网络训练是不利的，所有的 W 都朝着同一符号方向变化会减小训练速度，增加模型训练时间。
3.Sigmoid 函数包含 exp 指数运算，运算成本也比较大
```

### 4.2 tanh

<img src="https://s2.ax1x.com/2019/08/15/mAWyDO.png" alt="" border="0" />

<img src="https://s2.ax1x.com/2019/08/15/mAW2UH.png" alt="" border="0" />

tanh 函数的取值范围在 (-1,1) 之间，单调连续，求导容易。

相比于 Sigmoid 函数，tanh 函数的优点主要有两个：

1. 其一，收敛速度更快，如下图所示，tanh 函数线性区斜率较 Sigmoid 更大一些。在此区域内训练速度会更快。
2. 其二，tanh 函数输出均值为零，也就不存在 Sigmoid 函数中 dW 恒为正或者恒为负，从而影响训练速度的问题。

缺点：

1. tanh 函数与 Sigmoid 函数一样，也存在饱和区梯度消失问题。其饱和区甚至比 Sigmoid 还要大一些，但不明显。

### 4.3. ReLu

<img src="https://s2.ax1x.com/2019/08/15/mAfDij.png" alt="" border="0" />

<img src="https://s2.ax1x.com/2019/08/15/mAhiOf.png" alt="" border="0" />

优点：

1. 没有饱和区，不存在梯度消失问题。

2. 没有复杂的指数运算，计算简单、效率提高。

3. 实际收敛速度较快，大约是 Sigmoid/tanh 的 6 倍。

4. 比 Sigmoid 更符合生物学神经激活机制。



缺点：

1. ReLU 的输出仍然是非零对称的，可能出现 dW 恒为正或者恒为负，从而影响训练速度。
2. 当 x<0 时，ReLU 输出总为零。该神经元输出为零，则反向传播时，权重、参数的梯度横为零，造成权重、参数永远不会更新，即造成神经元失效，形成了“死神经元”。

**如何选择激活函数**

1. 首选 ReLU，速度快，但是要注意学习速率的调整，

2. 如果 ReLU 效果欠佳,尝试使用 Leaky ReLU、ELU 或 Maxout 等变种。

3. 可以尝试使用 tanh。

4. Sigmoid 和 tanh 在 RNN（LSTM、注意力机制等）结构中有所应用，作为门控或者概率值。其它情况下，减少 Sigmoid 的使用。

5. 在浅层神经网络中，选择使用哪种激励函数影响不大。

## 5. 神经网络的损失函数

常见的损失函数有均值函数（又叫二次代价函数），交叉熵函数

<img src="https://s2.ax1x.com/2019/08/15/mAhKlq.png" alt="" border="0">
<img src="https://s2.ax1x.com/2019/08/15/mAhM60.png" alt="" border="0">

用二次代价函数训练ANN，看到的实际效果是，如果误差越大，参数调整的幅度可能更小，训练更缓慢。

当误差越大，梯度就越大，参数w,b偏置项b 调整得越快，训练速度也就越快

## 6.  简单案例推导

<img src="https://s2.ax1x.com/2019/08/15/mAhJk4.png" alt="" border="0" height=300 width=500/>

以此图作为简单的案例推导

### 6.1正向传导

#### 6.1 .1输入层——>隐含层

**计算神经元的输入加权和**

<img src="https://s2.ax1x.com/2019/08/15/mAhvuV.png" alt="" border="0" height =50>

<img src="https://s2.ax1x.com/2019/08/15/mAhXj0.png" alt="" border="0" height=50 width=350>

**计算神经元的h1、h2的输出**

<img src="https://s2.ax1x.com/2019/08/15/mA41gI.png" alt="" border="0" height=100>

<img src="https://s2.ax1x.com/2019/08/15/mA4l8A.png" alt="" border="0" height =100>

#### 6.1.2  隐藏层——>输出层

**计算输出神经元o1、o2**

<img src="https://s2.ax1x.com/2019/08/15/mA4yrV.png" alt="mA4yrV.png" border="0">

<img src="https://s2.ax1x.com/2019/08/15/mA4sK0.png" alt="mA4sK0.png" border="0">

<img src="https://s2.ax1x.com/2019/08/15/mA4B2n.png" alt="mA4B2n.png" border="0" height=100 >

<img src="https://s2.ax1x.com/2019/08/15/mA4Dvq.png" alt="mA4Dvq.png" border="0" height = 100>

至此 前向传播到此结束

### 6.2反向传播

#### 6.2.1  **计算总误差**

<img src="https://s2.ax1x.com/2019/08/15/mA4jGd.jpg" alt="" border="0" height =300 width =600/>

#### 6.2.2  隐藏层——>输出层权值更新

<img src="https://s2.ax1x.com/2019/08/15/mA5Zzn.jpg" alt="" border="0" height =300 width =600/>

同理可求

<img src="https://s2.ax1x.com/2019/08/15/mA5ns0.jpg" alt="" border="0" height =300 width =600/>

**权值更新**

<img src="https://s2.ax1x.com/2019/08/15/mAxK3V.jpg" alt="" border="0" height =300 width =600 />

#### 6.2.3  隐藏层——>输入层权值更新

<img src="https://s2.ax1x.com/2019/08/15/mAx3B4.jpg" alt="" border="0" height =600 width =600/>

<img src="https://s2.ax1x.com/2019/08/15/mESmlV.jpg" alt="mESmlV.jpg" border="0" height =600 width =600/>

同理可求

<img src="https://s2.ax1x.com/2019/08/15/mAxYNR.jpg" alt="" border="0" height =600 width =600/>

权值更新

<img src="https://s2.ax1x.com/2019/08/15/mAxU9x.jpg" alt="mAxU9x.jpg" border="0" height =600 width =600/>

**权值更新后测试数据**

#### 6.2.4  偏执项b更新

**输出层——>隐藏层**

偏置项b由于他的系数是1，所以他的变化直接映射到误差的变化

<img src="https://s2.ax1x.com/2019/08/15/mESN6K.jpg" alt="" border="0" height width =600/>

## 7 .神经网络模型

### 7.1  前馈神经网络

前馈神经感知网络与感知机（FF or FFNN：Feed forward neural networks and P：perceptrons）非常简单，信息从前往后流动（分别对应输入和输出）。

一般在描述神经网络的时候，都是从它的层说起，即相互平行的输入层、隐含层或者输出层神经结构。单独的神经细胞层内部，神经元之间互不相连；而一般相邻的两个神经细胞层则是全连接（一层的每个神经元和另一层的每一个神经元相连）。一个最简单却最具有实用性的神经网络由两个输入神经元和一个输出神经元构成，也就是一个逻辑门模型。给神经网络一对数据集（分别是“输入数据集”和“我们期望的输出数据集”），一般通过反向传播算法来训练前馈神经网络（FFNNs）。

这就是所谓的监督式学习。与此相反的是无监督学习：我们只给输入，然后让神经网络去寻找数据当中的规律。反向传播的误差往往是神经网络当前输出和给定输出之间差值的某种变体（比如MSE或者仅仅是差值的线性变化）。如果神经网络具有足够的隐层神经元，那么理论上它总是能够建立输入数据和输出数据之间的关系。在实践中，FFNN的使用具有很大的局限性，但是，它们通常和其它神经网络一起组合成新的架构。

### 7.2  径向基神经网络（RBF）

径向神经网络（RBF：Radial basis function）是一种以径向基核函数作为激活函数的前馈神经网络。

### 7.3  霍普菲尔网络

霍普菲尔网络（HN：Hopfield network）是一种每一个神经元都跟其它神经元相互连接的网络。

这就像一盘完全搅在一起的意大利面，因为每个神经元都在充当所有角色：训练前的每一个节点都是输入神经元，训练阶段是隐神经元，输出阶段则是输出神经元。

该神经网络的训练，是先把神经元的值设置到期望模式，然后计算相应的权重。在这以后，权重将不会再改变了。一旦网络被训练包含一种或者多种模式，这个神经网络总是会收敛于其中的某一种学习到的模式，因为它只会在某一个状态才会稳定。值得注意的是，它并不一定遵从那个期望的状态（很遗憾，它并不是那个具有魔法的黑盒子）。它之所以会稳定下来，部分要归功于在训练期间整个网络的“能量（Energy）”或“温度（Temperature）”会逐渐地减少。每一个神经元的激活函数阈值都会被设置成这个温度的值，一旦神经元输入的总和超过了这个阈值，那么就会让当前神经元选择状态（通常是-1或1，有时也是0或1）。

可以多个神经元同步，也可以一个神经元一个神经元地对网络进行更新。一旦所有的神经元都已经被更新，并且它们再也没有改变，整个网络就算稳定（退火）了，那你就可以说这个网络已经收敛了。这种类型的网络被称为“联想记忆（associative memory）”，因为它们会收敛到和输入最相似的状态；比如，人类看到桌子的一半就可以想象出另外一半；与之相似，如果输入一半噪音+一半桌子，这个网络就能收敛到整张桌子。

### 7.4卷积神经网络

卷积神经网络是一种前馈型神经网络, 受生物自然视觉认知机制启发而来的. 现在, CNN 已经成为众多科学领域的研究热点之一, 特别是在模式分类领域, 由于该网络避免了对图像的复杂前期预处理, 可以直接输入原始图像, 因而得到了更为广泛的应用. 可应用于图像分类, 目标识别, 目标检测, 语义分割等等.

CNN基本组成结构

1. 输入层：数据的输入，但不是一维的向量，而是保留图片结构

2. 卷积层：使用固定的尺寸和权值的卷积核（也称滤波器），以一定的步长移动，对输入的矩阵进行滤波，加偏执，最后激活，最终组成一个特征图

3. 池化层：类比卷积核，以固定尺寸，一定步长移动，用最大池化或者平均池化对卷积层得到的特征进行差采样，加偏置，激活，组成一个更小的特征图

4. 全链接层：在CNN尾部进行拟合，减少特征值丢失

5. 输出层：输出结果

### 7.5  生成对抗网络

GAN生成式对抗网络（GAN：Generative adversarial networks）是一类不同的网络，它们有一对“双胞胎”：两个网络协同工作。

GAN可由任意两种网络组成（但通常是FF和CNN），其中一个用于生成内容，另一个则用于鉴别生成的内容。

鉴别网络（discriminating network）同时接收训练数据和生成网络（generative network）生成的数据。鉴别网络的准确率，被用作生成网络误差的一部分。这就形成了一种竞争：鉴别网络越来越擅长于区分真实的数据和生成数据，而生成网络也越来越善于生成难以预测的数据。这种方式非常有效，部分是因为：即便相当复杂的类噪音模式最终都是可预测的，但跟输入数据有着极为相似特征的生成数据，则很难区分。

训练GAN极具挑战性，因为你不仅要训练两个神经网络（其中的任何一个都会出现它自己的问题），同时还要平衡两者的运行机制。如果预测或生成相比对方表现得过好，这个GAN就不会收敛，因为它会内部发散。